report: "report/workflow.rst"

include: "rules/basic.smk"
configfile: "config/config.yaml"
import pandas as pd
import numpy as np
from pathlib import Path
# generate centralized sample datasets
bgcflow_util_dir = Path("data/interim/bgcflow_utils")
bgcflow_util_dir.mkdir(parents=True, exist_ok=True)

def extract_custom_sample_information(custom_samples_path):
    df = pd.read_csv(custom_samples_path, index_col=0, header=0, low_memory=False)
    df.index.name = "genome_id"
    if not "path" in df.columns:
        df["path"] = np.nan
    df.loc[df["path"] == "", "path"] = np.nan
    df["source"] = "local"
    df.loc[pd.isna(df["path"]), "source"] = "ncbi"
    return df
    
def extract_project_information(config):
    # load information from config
    print(f"Step 1. Extracting project information from config...\n", file=sys.stderr)
    print(config)
    taxons = config["taxons"]

    taxon_records = []
    custom_samples_df = None

    for num, p in enumerate(taxons):
        print(
            f"Step 2.{num+1} Getting sample information from: {p['name']}",
            file=sys.stderr,
        )
        custom_samples_file = p.get("custom_samples", None)
        if not custom_samples_file is None:
            print(f"Step 3.{num+1} Extracting custom sample information from: {custom_samples_file}.")
            custom_samples_df = pd.concat([custom_samples_df, extract_custom_sample_information(custom_samples_file)])
        # grab a bgcflow pep project
        record = {
            "name": p["name"].replace(" ", "_"),
            "taxon": p["name"].replace("_", " "),
            "reference_only": p.get("reference_only", True),
            "custom_samples_file": custom_samples_file,
        }
        taxon_records.append(record)
    taxons_df = pd.DataFrame.from_records(taxon_records, index="name")
    return taxons_df, custom_samples_df

##### 1. Extract information from config file
TAXONS, CUSTOM_SAMPLES = extract_project_information(config)

KINGDOM = "bacteria"

# TAXONS = extract_project_information(config)

GTDB_PATHS = []
CUSTOM_FNA = []
STRAINS_FNA = ["Dummy"]
PROKKA_DB_MAP = {}

RULE_FUNCTIONS = {}

##### 3. Wildcard constraints #####
wildcard_constraints:
    assembly_source="all|RefSeq|GenBank",
    taxon="|".join(TAXONS.index.to_list()),

include: "rules/preselect_genomes/preselect_all_genomes.smk"
include: "rules/select_genomes/select_all_genomes.smk"

def get_taxons():
    return TAXONS.index.to_list()
def get_accessions_for_name(name):
    return get_accessions_for_taxon(name)
# def get_fasta_inputs_for_name(name):
#     return [f"data/interim/fasta/{s}.fna" for s in get_accessions_for_name(name)]
# def filter_samples_qc(wildcards, df_samples):
#     qc = pd.read_csv(checkpoints.qc.get().output[0], sep=",", index_col=0, header=0)
#     df_samples_names = set(df_samples.index.to_list())
#     # assert (df_samples_names == qc_samples_names)
#     qc_samples_names = set(qc.index[qc["passed"] == True].to_list())
#     sel = list(df_samples_names & qc_samples_names)
#     return df_samples.loc[sel, :]
def filter_sample_names_qc(sample_names):
    sample_names = set(sample_names)
    qc = pd.read_csv(checkpoints.qc.get().output[0], sep=",", index_col=0, header=0)
    qc_samples_names = set(qc.index[qc["passed"] == True].to_list())
    sel = list(sample_names & qc_samples_names)
    return sel
RULE_FUNCTIONS["qc"] = {"names": get_taxons}
include: "rules/qc/qc_contigs_n50.smk"

resource_mapping = {}
#### Modules #####
include: "rules/custom_genomes.smk"
include: "rules/ncbi_datasets.smk"

##### 4. Generate user-defined local resources
# custom_resource_dir(config["resources_path"], resource_mapping)

RULE_FUNCTIONS["seqfu"] = {"strains": get_accessions_for_taxon}
include: "rules/seqfu.smk"
include: "rules/checkm_for_missing.smk"
RULE_FUNCTIONS["checkm"] = {"accessions": get_accessions_for_taxon}
include: "rules/checkm.smk"

RULE_FUNCTIONS["gtdbtk_simple"] = {"accessions": get_unclassified_accessions}
include: "rules/gtdbtk_simple.smk"

RULE_FUNCTIONS["gtdb_improved"] = {"taxons": get_taxons}
include: "rules/gtdb_improved.smk"
# include: "rules/merge_taxonomy/gtdb_only.smk"
include: "rules/merge_taxonomy/gtdb_and_gtdbtk.smk"
include: "rules/split_species_into_projects.smk"
# get_samples_df = get_species_projects_samples_df
# get_projects = get_species_projects
# get_samples_for_project = get_samples_for_species_project
# get_samples_df_for_project = get_samples_for_species_project

def filtered_accessions_for_project(name):
    return filter_sample_names_qc(get_samples_for_species_project(name))

RULE_FUNCTIONS["prokka"] = {"samples": get_species_projects_samples_df}
include: "rules/prokka.smk"
RULE_FUNCTIONS["roary"] = {"samples": filtered_accessions_for_project}
include: "rules/roary.smk"
RULE_FUNCTIONS["mash"] = {"accessions": filtered_accessions_for_project}
include: "rules/mash.smk"
RULE_FUNCTIONS["automlst_wrapper"] = {"accessions": filtered_accessions_for_project}
include: "rules/automlst_wrapper.smk"
RULE_FUNCTIONS["alleleome"] = {"samples": filtered_accessions_for_project}
include: "rules/alleleome.smk"
RULE_FUNCTIONS["pankb_data_prep"] = {"projects": get_species_projects, "genomes": filtered_accessions_for_project}
include: "rules/pankb_data_prep.smk"

rule all:
    input:
        # "data/interim/ncbi_datasets/taxon/Moraxellaceae.tsv",
        # "data/interim/ncbi_datasets/taxon/Moraxellaceae.dummy",
        # "data/interim/fasta/GCF_000196795.1.fna",
        # expand("data/interim/ncbi_datasets/taxon/{taxon}.csv", taxon=get_taxons()),
        # expand("data/interim/ncbi_datasets/taxon/{taxon}.dummy", taxon=TAXONS.index.to_list()),
        # expand("data/interim/fasta/{accession}.fna", accession=["GCF_000196795.1"]),
        # lambda _: expand("data/interim/fasta/{accession}.fna", accession=get_all_accessions()),
        # lambda _: expand("data/interim/gtdb/{accession}.json", accession=get_all_accessions()),
        # expand("data/processed/{taxon}/tables/df_gtdb_meta.csv", taxon=get_taxons()),
        # expand("data/processed/{taxon}/tables/gtdbtk.bac120.summary.tsv", taxon=TAXONS.index.to_list()),
        # "data/processed/qc/qc_passed.csv",
        # expand("data/interim/split/{taxon}/classification.csv", taxon=get_taxons()),
        # expand("data/interim/gtdb/{taxon}.jsonl", taxon=TAXONS.index.to_list()),
        # expand("data/interim/checkm/{taxon}_missing.txt", taxon=TAXONS.index.to_list()),
        lambda _: expand("data/processed/{name}/alleleome/Pan/dn_ds.json", name=get_species_projects()),
	    # "data/interim/alleleome/Lacticaseibacillus_paracasei/pangenome_alignments/process_dummy_Pan",
        lambda _: expand("data/processed/{name}/pankb/genome_page/", name=get_species_projects()),
        lambda _: expand("data/processed/{name}/pankb/gene_locustag/", name=get_species_projects()),
        lambda _: expand("data/processed/{name}/pankb/gene_freq.json", name=get_species_projects()),
        lambda _: expand("data/processed/{name}/pankb/COG_distribution.json", name=get_species_projects()),
        lambda _: expand("data/processed/{name}/pankb/All.json", name=get_species_projects()),
        lambda _: expand("data/processed/{name}/pankb/heatmap_target.json", name=get_species_projects()),

